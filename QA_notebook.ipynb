{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "36aba9d4-0395-4a6c-aae6-6cca97064fec",
   "metadata": {},
   "source": [
    "Q-A model搭建\n",
    "\n",
    "1. 下载中QA模型：`roberta-base-chinese-extractive-qa` : https://hf-mirror.com/uer/roberta-base-chinese-extractive-qa\n",
    "2. 使用数据集对Q-A模型进行训练\n",
    "3. 部署(flask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6630b94-87e3-49c9-9e0b-4622643bf043",
   "metadata": {},
   "source": [
    "## 获取QA模型\n",
    "加载模型并测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6326d123-2d18-48b8-af00-a87b2afd1b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "from transformers import AutoModelForQuestionAnswering, AutoTokenizer, pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f5d072-f766-42d5-b31f-46bfff4fc94a",
   "metadata": {},
   "outputs": [],
   "source": [
    "QA_model = pipeline(\"question-answering\", \n",
    "                    model=\"./models_roberta-base-chinese-extractive-qa\",\n",
    "                    tokenizer=\"./models_roberta-base-chinese-extractive-qa\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74698ef0-0623-41bf-97e6-ebed5175bfdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "QA_input = {'question': \"著名诗歌《假如生活欺骗了你》的作者是\",\n",
    "            'context': \"普希金从那里学习人民的语言，吸取了许多有益的养料，这一切对普希金后来的创作产生了很大的影响。这两年里，普希金创作了不少优秀的作品，如《囚徒》、《致大海》、《致凯恩》和《假如生活欺骗了你》等几十首抒情诗，叙事诗《努林伯爵》，历史剧《鲍里斯·戈都诺夫》，以及《叶甫盖尼·奥涅金》前六章。\"}\n",
    "QA_model(QA_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d017888-e631-45cd-ba54-54c9523c6f29",
   "metadata": {},
   "source": [
    "## fine-tuning 模型\n",
    "\n",
    "可以参考如下 `notebook` 或 `qa_training_v0.py`\n",
    "\n",
    "```sh\n",
    "\n",
    "#把training 数据 和 模型放到对应文件夹下\n",
    "\n",
    " nohup python qa_training_v0.py &\r\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb36709f-ce95-487f-8b55-c70888b6a5cc",
   "metadata": {},
   "source": [
    "### 加载pre-train 模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d423cf-93bf-4e20-a82c-f1eb7217c7cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
    "model_name = \"./models_roberta-base-chinese-extractive-qa/\"  # 使用roberta-base-squad2模型\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4df2bf1-8fe2-4285-89a9-f14828e15709",
   "metadata": {},
   "source": [
    "### 数据预处理\n",
    "\n",
    "1. 将csv --> squad.json\r",
    "2. \n",
    "将 squad.json -> 可输入的训练\n",
    "3. \r\n",
    "训练数据"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "396cbf61-d9d4-4e46-9ad8-b2e684d43c44",
   "metadata": {},
   "source": [
    "#### 将csv转换为SQuAD 格式的文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8eae376-b9d7-4b4f-96ea-e0e54b26972d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import json\n",
    "\n",
    "def convert_to_squad_format(input_file, output_file):\n",
    "    squad_data = {\"data\": []}\n",
    "    \n",
    "    with open(input_file, newline='', encoding='utf-8') as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        \n",
    "        paragraphs = []\n",
    "        for row in reader:\n",
    "            context = row['context']\n",
    "            question = row['question']\n",
    "            answer_text = row['answers']\n",
    "            \n",
    "            # 找到答案在 context 中的起始位置\n",
    "            answer_start = context.find(answer_text)\n",
    "            \n",
    "            if answer_start == -1:\n",
    "                raise ValueError(f\"Answer '{answer_text}' not found in context '{context}'\")\n",
    "            \n",
    "            # 构建符合 SQuAD 格式的结构\n",
    "            qas = {\n",
    "                \"question\": question,\n",
    "                \"id\": str(hash(question)),\n",
    "                \"answers\": [{\n",
    "                    \"text\": answer_text,\n",
    "                    \"answer_start\": answer_start\n",
    "                }],\n",
    "                \"is_impossible\": False\n",
    "            }\n",
    "            \n",
    "            paragraph = {\n",
    "                \"context\": context,\n",
    "                \"qas\": [qas]\n",
    "            }\n",
    "            \n",
    "            paragraphs.append(paragraph)\n",
    "        \n",
    "        # 将段落添加到 \"data\" 部分\n",
    "        squad_data[\"data\"].append({\n",
    "            \"title\": \"custom_dataset\",\n",
    "            \"paragraphs\": paragraphs\n",
    "        })\n",
    "    \n",
    "    # 将结果写入 JSON 文件\n",
    "    with open(output_file, 'w', encoding='utf-8') as outfile:\n",
    "        json.dump(squad_data, outfile, ensure_ascii=False, indent=4)\n",
    "\n",
    "# 使用示例\n",
    "convert_to_squad_format(\"QA_test_data/test.csv\", \"QA_test_data/test_squad_format.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff6e617-14d0-48b9-86f5-3d7e5c229dd1",
   "metadata": {},
   "source": [
    "#### 加载SQUAD数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bbd954c-4f05-4060-9a3e-bad2a06d9cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载 SQuAD 格式的 JSON 数据集\n",
    "with open(\"QA_test_data/test_squad_format.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    squad_data = json.load(f)\n",
    "\n",
    "# 准备数据\n",
    "contexts = []\n",
    "questions = []\n",
    "answers_text = []\n",
    "answers_start = []\n",
    "\n",
    "# 遍历数据集，将其转化为需要的格式\n",
    "for article in squad_data['data']:\n",
    "    for paragraph in article['paragraphs']:\n",
    "        context = paragraph['context']\n",
    "        for qa in paragraph['qas']:\n",
    "            question = qa['question']\n",
    "            for answer in qa['answers']:\n",
    "                contexts.append(context)\n",
    "                questions.append(question)\n",
    "                answers_text.append(answer['text'])\n",
    "                answers_start.append(answer['answer_start'])\n",
    "\n",
    "# 将数据加载为 Dataset 对象\n",
    "data_dict = {\n",
    "    'context': contexts,\n",
    "    'question': questions,\n",
    "    'answers': [{'text': a, 'answer_start': b} for a, b in zip(answers_text, answers_start)]\n",
    "}\n",
    "\n",
    "dataset = Dataset.from_dict(data_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a430b86-249c-48bc-8e92-c8a7c7f6435b",
   "metadata": {},
   "source": [
    "#### 将questions和context进行tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b15e3bd3-1965-4329-90ff-f0405e5bc075",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    # 提取 question 和 context\n",
    "    questions = [q.strip() for q in examples[\"question\"]]\n",
    "    contexts = examples[\"context\"]\n",
    "\n",
    "    # 对 question 和 context 进行批量编码，设置最大长度和 padding\n",
    "    # 只需要对qeustion和context进行tokenizer就可以了？\n",
    "    inputs = tokenizer(\n",
    "        questions,\n",
    "        contexts,\n",
    "        truncation=\"only_second\",  # 在 context 进行截断\n",
    "        max_length=512,  # 设置最大长度为 512\n",
    "        padding=\"max_length\",  # 使用最大长度进行填充\n",
    "        return_offsets_mapping=True,  # 返回 offsets 以计算答案的位置\n",
    "        return_tensors=\"np\"  # 返回 NumPy 格式，确保所有长度一致\n",
    "    )\n",
    "\n",
    "    # 为了保存答案的起始和结束 token 索引，我们手动计算\n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "\n",
    "    for i in range(len(examples[\"answers\"])):\n",
    "        # 获取答案的开始字符位置\n",
    "        answer_start = examples[\"answers\"][i][\"answer_start\"]\n",
    "        answer_text = examples[\"answers\"][i][\"text\"]\n",
    "\n",
    "        # 获取 context 的 offset 映射\n",
    "        offset_mapping = inputs[\"offset_mapping\"][i]\n",
    "        input_ids = inputs[\"input_ids\"][i]\n",
    "\n",
    "        # 查找答案的起始和结束 token 索引\n",
    "        start_char = answer_start\n",
    "        end_char = start_char + len(answer_text)\n",
    "\n",
    "        # 初始化 token 索引\n",
    "        token_start_index = 0\n",
    "        token_end_index = 0\n",
    "\n",
    "        # 查找与字符索引对应的 token 索引\n",
    "        for idx, (start, end) in enumerate(offset_mapping):\n",
    "            if start <= start_char < end:\n",
    "                token_start_index = idx\n",
    "            if start <= end_char <= end:\n",
    "                token_end_index = idx\n",
    "                break\n",
    "\n",
    "        # 保存 token 索引\n",
    "        start_positions.append(token_start_index)\n",
    "        end_positions.append(token_end_index)\n",
    "\n",
    "    # 移除 offset_mapping 因为我们已经不再需要它\n",
    "    inputs.pop(\"offset_mapping\")\n",
    "\n",
    "    # 添加 start_positions 和 end_positions\n",
    "    inputs[\"start_positions\"] = start_positions\n",
    "    inputs[\"end_positions\"] = end_positions\n",
    "\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff39fe27-50c3-4068-ac46-43b0bbbe6e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset = dataset.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a13127fb-46a0-45ab-aa9b-c68b0ebb44e8",
   "metadata": {},
   "source": [
    "### 训练模型\n",
    "\n",
    "使用以上数据集，对模型进行训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b061a934-76b0-4760-8075-9b1da17907fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',          # 输出目录\n",
    "    evaluation_strategy=\"epoch\",     # 每轮评估一次\n",
    "    learning_rate=3e-4,              # 学习率\n",
    "    per_device_train_batch_size=8,  # 每个设备的batch size\n",
    "    per_device_eval_batch_size=8,   # 验证的batch size\n",
    "    num_train_epochs=3,              # 训练轮数\n",
    "    weight_decay=0.01,               # 权重衰减\n",
    "    logging_dir='./logs',            # 日志目录\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    eval_dataset=tokenized_dataset,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ee8414-0372-45bf-8cdb-956cb27aa857",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a852343d-5b48-4a6a-af31-f76c95da5260",
   "metadata": {},
   "source": [
    "#### 模型保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba255668-13ca-4f98-a3f3-1f022be9596e",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(\"./tmp/finetuned-roberta-base-squad2_wuxi2\")\n",
    "tokenizer.save_pretrained(\"./tmp/finetuned-roberta-base-squad2_wuxi2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d42d9785-114a-4a5d-91f8-1adac5321e93",
   "metadata": {},
   "source": [
    "#### 模型加载与使用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f43707-bfde-46a9-98da-8603c615f17b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = pipeline(\"question-answering\", \n",
    "                 model=\"./tmp/finetuned-roberta-base-squad2_wuxi2\",\n",
    "                 tokenizer=\"./tmp/finetuned-roberta-base-squad2_wuxi2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9510a8a0-6c63-42f3-9a15-5c4d3d954480",
   "metadata": {},
   "outputs": [],
   "source": [
    "QA_input = {\n",
    "    'question': '查什么模型',\n",
    "    'context': '帮忙查下PDX模型中EGFR的表达'\n",
    "}\n",
    "res = model(QA_input)\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff359a74-b0d0-41dc-961c-416bb77c4016",
   "metadata": {},
   "source": [
    "## 模型部署\n",
    "\n",
    "1. 本地部署\n",
    "2. flask云端部署"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7273cdb-40a8-4eda-9479-e1b3c9545040",
   "metadata": {},
   "source": [
    "### 本地部署\n",
    "\n",
    "将模型进行本地部署，使用脚本调用模型：\n",
    "\n",
    "```sh\n",
    "python qa_search_v1.py --question 在我们PDX模型中检索EGFR突变数据\n",
    "#输出固定的检索结果为 `res.tsv`\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f395537-e1bd-45d6-95c1-7bf0830b040c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#! /home_bk/fan_qiangqiang/miniconda3/envs/chatGLM/bin/python\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on 2024.10.16\n",
    "@author: fan_qiangqiang\n",
    "Description: 在PDX，细胞系，Syngeneic模型中，检索基因的表达、突变、基因融合，拷贝数变异等数据\n",
    "\"\"\"\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering,pipeline\n",
    "import subprocess\n",
    "import argparse \n",
    "\n",
    "def get_dict(filein):\n",
    "    '获取model_types 和 datatypes'\n",
    "    dict_map={}\n",
    "    with open(filein,'r') as f:\n",
    "        fs=f.readlines()\n",
    "        for rs in fs:\n",
    "            records=rs.split(\"\\t\")\n",
    "            dict_map[records[0]]=records[1].strip(\"\\n\")\n",
    "    return dict_map\n",
    "    \n",
    "\n",
    "def search(models,genes,datatypes):\n",
    "    '根据输入的models，genes和datatypes，在服务器后台进行检索'\n",
    "    dt=datatype_dict[datatypes]\n",
    "    genes=\"|\".join([m.upper() for m in genes.split(\",\")])\n",
    "\n",
    "    #还需要先评价下是否有空值\n",
    "    \n",
    "    if dt=='mutation':\n",
    "        search_mutation(models,genes,dt)\n",
    "        \n",
    "    if dt=='expression':\n",
    "        search_expression(models,genes,dt)\n",
    "        \n",
    "    if dt=='CNV':\n",
    "        search_CNV(models,genes,dt)\n",
    "        \n",
    "    if dt=='fusion':\n",
    "        search_fusion(models,genes,dt)\n",
    "        \n",
    "    if dt=='HLA':\n",
    "        search_HLA(models,genes,dt)\n",
    "\n",
    "\n",
    "def search_mutation(models,genes,dt):\n",
    "    '检索突变'\n",
    "    command=\"cat {NGSpath}*|grep -E exonic|grep -wiE '{genes}'\".format(\n",
    "        NGSpath=getattr(tmp_database[model_dict[models]],dt),\n",
    "        genes= genes)\n",
    "    #print(command)\n",
    "    result = subprocess.Popen(command,shell=True,\n",
    "                             stdout=subprocess.PIPE, \n",
    "                             stderr=subprocess.PIPE, \n",
    "                             text=True)\n",
    "    stdout, stderr = result.communicate()\n",
    "    with open('res.tsv','w') as f:\n",
    "        f.writelines(\"\\t\".join([\"Model ID\",\"WGC ID\",\"chroM\",\"Start\",\"End\",\"ref\",\"Alt\",\n",
    "                               \"Genotype\", \"GeneLocation\",\"Gene\",\"Exon Syno\",\n",
    "                               \"Exon\",\"dbSNP\", \"1000G\",\"Cosmic\",\"AF\",\"DP\"])+\"\\n\")\n",
    "        f.writelines(stdout)\n",
    "\n",
    "def search_expression(models,genes,dt):\n",
    "    '检索表达'\n",
    "    command=\"cat {NGSpath}*|grep -wiE '{genes}'|cut -f1,2,5\".format(\n",
    "        NGSpath=getattr(tmp_database[model_dict[models]],dt),\n",
    "        genes= genes)\n",
    "    #print(command)\n",
    "    result = subprocess.Popen(command,shell=True,\n",
    "                             stdout=subprocess.PIPE, \n",
    "                             stderr=subprocess.PIPE, \n",
    "                             text=True)\n",
    "    stdout, stderr = result.communicate()\n",
    "    with open('res.tsv','w') as f:\n",
    "        f.writelines(\"\\t\".join(['modelID','Gene','expression'])+\"\\n\")\n",
    "        f.writelines(stdout)\n",
    "\n",
    "def search_CNV(models,genes,dt):\n",
    "    '检索CNV'\n",
    "    command=\"cat {NGSpath}*|grep -wiE '{genes}'|cut -f1,2,5\".format(\n",
    "        NGSpath=getattr(tmp_database[model_dict[models]],dt),\n",
    "        genes= genes)\n",
    "    #print(command)\n",
    "    result = subprocess.Popen(command,shell=True,\n",
    "                             stdout=subprocess.PIPE, \n",
    "                             stderr=subprocess.PIPE, \n",
    "                             text=True)\n",
    "    stdout, stderr = result.communicate()\n",
    "    with open('res.tsv','w') as f:\n",
    "        f.writelines(\"\\t\".join([\"Model ID\",\"Gene\",\"CNV\"])+\"\\n\")\n",
    "        f.writelines(stdout)\n",
    "\n",
    "def search_fusion(models,genes,dt):\n",
    "    '检索融合'\n",
    "    command=\"cat {NGSpath}*|grep -wiE '{genes}' | cut -f1,2,7,8,9,10,14\".format(\n",
    "        NGSpath=getattr(tmp_database[model_dict[models]],dt),\n",
    "        genes= genes)\n",
    "    #print(command)\n",
    "    result = subprocess.Popen(command,shell=True,\n",
    "                             stdout=subprocess.PIPE, \n",
    "                             stderr=subprocess.PIPE, \n",
    "                             text=True)\n",
    "    stdout, stderr = result.communicate()\n",
    "    with open('res.tsv','w') as f:\n",
    "        f.writelines(\"\\t\".join([\"ID\",\"JunctionReadCount\",\"LeftGene\",\n",
    "                                \"LeftBreakpoint\",\"RightGene\",\"RightBreakpoint\",\"FFPM\"])+\"\\n\")\n",
    "        f.writelines(stdout)\n",
    "\n",
    "def search_HLA(models,genes,dt):\n",
    "    pass\n",
    "\n",
    "def get_inputs(context_query):\n",
    "    '从输入中获取模型，基因，数据类型'    \n",
    "    QA_input={'question':'什么模型','context':context_query}\n",
    "    global model\n",
    "    res = model(QA_input)\n",
    "    models=res['answer'] if res['score'] >0.9 else ''\n",
    "    \n",
    "    QA_input={'question':'什么数据类型','context':context_query}\n",
    "    res = model(QA_input)\n",
    "    datatypes=res['answer'] if res['score'] >0.9 else '' \n",
    "    \n",
    "    QA_input={'question':'什么基因','context':context_query}\n",
    "    res = model(QA_input)\n",
    "    genes=res['answer'] if res['score'] >0.9 else ''\n",
    "    \n",
    "    return models,genes,datatypes\n",
    "\n",
    "def write_out(log):\n",
    "    '输出检索日志'\n",
    "    with open('QA_log','a') as f:\n",
    "        f.writelines(log+\"\\n\")\n",
    "\n",
    "class innerDatabse:\n",
    "    def __init__(self,name,expression,mutation,CNV,HLA,fusion):\n",
    "        self.name=name\n",
    "        self.expression=expression\n",
    "        self.mutation=mutation\n",
    "        self.CNV=CNV\n",
    "        self.HLA=HLA\n",
    "        self.fusion=fusion\n",
    "\n",
    "tmp_database={}\n",
    "tmp_database['PDX']=innerDatabse(name='PDX',\n",
    "                                 expression='/OIU/innerNGSresults/filterRNAseq/',\n",
    "                                mutation='/OIU/innerNGSresults/filterWXS/',\n",
    "                                CNV='/OIU/innerNGSresults/CNV_cnvkit/',\n",
    "                                HLA='/OIU/innerNGSresults/HLA/',\n",
    "                                fusion='/OIU/innerNGSresults/fusion_starfusion_formatPDX/')\n",
    "tmp_database['cancer_cell_line']=innerDatabse(name='cancer cell line',\n",
    "                                 expression='/OIU/innerNGSresults/CellLineDatasets/expression_format_TPM/',\n",
    "                                mutation='/OIU/innerNGSresults/CellLineDatasets/mutation_format/',\n",
    "                                CNV='/OIU/innerNGSresults/CellLineDatasets/cnv_format/',\n",
    "                                HLA='/OIU/innerNGSresults/CellLineDatasets/HLA_HD/',\n",
    "                                fusion='/OIU/innerNGSresults/CellLineDatasets/fusion_starfusion/')\n",
    "\n",
    "tmp_database['syngeneic']=innerDatabse(name='syngeneic models',\n",
    "                                 expression='/OIU/innerNGSresults/Syngeneic/expression/',\n",
    "                                mutation='/OIU/innerNGSresults/Syngeneic/mutation/',\n",
    "                                CNV='',\n",
    "                                HLA='',\n",
    "                                fusion='')\n",
    "                                \n",
    "model = pipeline(\"question-answering\", \n",
    "                 model=\"./tmp/finetuned-roberta-base-squad2_wuxi1\",\n",
    "                 tokenizer=\"./tmp/finetuned-roberta-base-squad2_wuxi1\")\n",
    "model_dict=get_dict('./model_dict')\n",
    "datatype_dict=get_dict('./datatype_dict')\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Command-line argument parser\n",
    "    parser = argparse.ArgumentParser(description=\"search NGS data based on your input\")\n",
    "    parser.add_argument('--question', type=str, required=True, help=\"your query question: 在我们PDX模型中检索EGFR突变数据\")\n",
    "    #parser.add_argument('--param2', type=int, default=20, help=\"Second parameter (default: 20)\")\n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    # Use the example function\n",
    "    input_contest=args.question\n",
    "    print(\"Searching....\")\n",
    "    assert input_contest, \"Your input not avaliable, please check your question\"\n",
    "    models,genes,datatypes=get_inputs(input_contest)\n",
    "    \n",
    "    res = 1 if models in model_dict and genes and datatypes in datatype_dict else 0\n",
    "    if res:\n",
    "        search(models,genes,datatypes)\n",
    "        check_status=\"\\t\".join([input_contest,datatype_dict[datatypes],models,genes,datatypes,str(res)])\n",
    "        write_out(check_status)\n",
    "        print(\"Complete! Please find results in res.tsv\")\n",
    "    else:\n",
    "        check_status=\"\\t\".join([input_contest,'-',models,genes,datatypes,str(res)])\n",
    "        write_out(check_status)\n",
    "        print(\"No data feedback!\")\n",
    "    \n",
    "# Ensure the script runs only when executed directly\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "537e13d4-66a3-4bfc-8f11-271a8dd1c77e",
   "metadata": {},
   "source": [
    "### flask 云端部署\n",
    "\n",
    "1. 云端部署，只需要API即可访问使用: `qa_search_v2.py`\n",
    "2. 加上uuid，可以多人同时使用：`qa_search_v3.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bdc8841-47bb-4ea9-8ed5-759b6455f6d1",
   "metadata": {},
   "source": [
    "#### 服务器端脚本\n",
    "\n",
    "`qa_search_v3.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b44bbec-a3bf-441d-98ff-286d49a513d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#! /home_bk/fan_qiangqiang/miniconda3/envs/chatGLM/bin/python\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on 2024.10.16\n",
    "@author: fan_qiangqiang\n",
    "Description: 在PDX，细胞系，Syngeneic模型中，检索基因的表达、突变、基因融合，拷贝数变异等数据\n",
    "Update: \n",
    "1. 更新为API使用\n",
    "2. 多用户使用是，分别分发结果\n",
    "\"\"\"\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering,pipeline\n",
    "import subprocess\n",
    "import argparse\n",
    "from flask import Flask, request, jsonify, send_file\n",
    "import uuid\n",
    "\n",
    "def get_dict(filein):\n",
    "    '获取model_types 和 datatypes'\n",
    "    dict_map={}\n",
    "    with open(filein,'r') as f:\n",
    "        fs=f.readlines()\n",
    "        for rs in fs:\n",
    "            records=rs.split(\"\\t\")\n",
    "            dict_map[records[0]]=records[1].strip(\"\\n\")\n",
    "    return dict_map\n",
    "\n",
    "\n",
    "def search(models,genes,datatypes,outs):\n",
    "    '根据输入的models，genes和datatypes，在服务器后台进行检索'\n",
    "    dt=datatype_dict[datatypes]\n",
    "    genes=\"|\".join([m.upper() for m in genes.split(\",\")])\n",
    "\n",
    "    #还需要先评价下是否有空值\n",
    "\n",
    "    if dt=='mutation':\n",
    "        search_mutation(models,genes,dt,outs)\n",
    "\n",
    "    if dt=='expression':\n",
    "        search_expression(models,genes,dt,outs)\n",
    "\n",
    "    if dt=='CNV':\n",
    "        search_CNV(models,genes,dt,outs)\n",
    "\n",
    "    if dt=='fusion':\n",
    "        search_fusion(models,genes,dt,outs)\n",
    "\n",
    "    if dt=='HLA':\n",
    "        search_HLA(models,genes,dt,outs)\n",
    "\n",
    "\n",
    "def search_mutation(models,genes,dt,outs):\n",
    "    '检索突变'\n",
    "    command=\"cat {NGSpath}*|grep -E exonic|grep -wiE '{genes}'\".format(\n",
    "        NGSpath=getattr(tmp_database[model_dict[models]],dt),\n",
    "        genes= genes)\n",
    "    #print(command)\n",
    "    result = subprocess.Popen(command,shell=True,\n",
    "                             stdout=subprocess.PIPE,\n",
    "                             stderr=subprocess.PIPE,\n",
    "                             text=True)\n",
    "    stdout, stderr = result.communicate()\n",
    "    with open(outs,'w') as f:\n",
    "        f.writelines(\"\\t\".join([\"Model ID\",\"WGC ID\",\"chroM\",\"Start\",\"End\",\"ref\",\"Alt\",\n",
    "                               \"Genotype\", \"GeneLocation\",\"Gene\",\"Exon Syno\",\n",
    "                               \"Exon\",\"dbSNP\", \"1000G\",\"Cosmic\",\"AF\",\"DP\"])+\"\\n\")\n",
    "        f.writelines(stdout)\n",
    "\n",
    "def search_expression(models,genes,dt,outs):\n",
    "    '检索表达'\n",
    "    command=\"cat {NGSpath}*|grep -wiE '{genes}'|cut -f1,2,5\".format(\n",
    "        NGSpath=getattr(tmp_database[model_dict[models]],dt),\n",
    "        genes= genes)\n",
    "    #print(command)\n",
    "    result = subprocess.Popen(command,shell=True,\n",
    "                             stdout=subprocess.PIPE,\n",
    "                             stderr=subprocess.PIPE,\n",
    "                             text=True)\n",
    "    stdout, stderr = result.communicate()\n",
    "    with open(outs,'w') as f:\n",
    "        f.writelines(\"\\t\".join(['modelID','Gene','expression'])+\"\\n\")\n",
    "        f.writelines(stdout)\n",
    "\n",
    "def search_CNV(models,genes,dt,outs):\n",
    "    '检索CNV'\n",
    "    command=\"cat {NGSpath}*|grep -wiE '{genes}'|cut -f1,2,5\".format(\n",
    "        NGSpath=getattr(tmp_database[model_dict[models]],dt),\n",
    "        genes= genes)\n",
    "    #print(command)\n",
    "    result = subprocess.Popen(command,shell=True,\n",
    "                             stdout=subprocess.PIPE,\n",
    "                             stderr=subprocess.PIPE,\n",
    "                             text=True)\n",
    "    stdout, stderr = result.communicate()\n",
    "    with open(outs,'w') as f:\n",
    "        f.writelines(\"\\t\".join([\"Model ID\",\"Gene\",\"CNV\"])+\"\\n\")\n",
    "        f.writelines(stdout)\n",
    "\n",
    "def search_fusion(models,genes,dt,outs):\n",
    "    '检索融合'\n",
    "    command=\"cat {NGSpath}*|grep -wiE '{genes}' | cut -f1,2,7,8,9,10,14\".format(\n",
    "        NGSpath=getattr(tmp_database[model_dict[models]],dt),\n",
    "        genes= genes)\n",
    "    #print(command)\n",
    "    result = subprocess.Popen(command,shell=True,\n",
    "                             stdout=subprocess.PIPE,\n",
    "                             stderr=subprocess.PIPE,\n",
    "                             text=True)\n",
    "    stdout, stderr = result.communicate()\n",
    "    with open(outs,'w') as f:\n",
    "        f.writelines(\"\\t\".join([\"ID\",\"JunctionReadCount\",\"LeftGene\",\n",
    "                                \"LeftBreakpoint\",\"RightGene\",\"RightBreakpoint\",\"FFPM\"])+\"\\n\")\n",
    "        f.writelines(stdout)\n",
    "\n",
    "def search_HLA(models,genes,dt):\n",
    "    pass\n",
    "\n",
    "def get_inputs(context_query):\n",
    "    '从输入中获取模型，基因，数据类型'\n",
    "    QA_input={'question':'什么模型','context':context_query}\n",
    "    global model\n",
    "    res = model(QA_input)\n",
    "    models=res['answer'] if res['score'] >0.9 else ''\n",
    "\n",
    "    QA_input={'question':'什么数据类型','context':context_query}\n",
    "    res = model(QA_input)\n",
    "    datatypes=res['answer'] if res['score'] >0.9 else ''\n",
    "\n",
    "    QA_input={'question':'什么基因','context':context_query}\n",
    "    res = model(QA_input)\n",
    "    genes=res['answer'] if res['score'] >0.9 else ''\n",
    "\n",
    "    return models,genes,datatypes\n",
    "\n",
    "def write_out(log):\n",
    "    '输出检索日志'\n",
    "    with open('QA_log','a') as f:\n",
    "        f.writelines(log+\"\\n\")\n",
    "\n",
    "class innerDatabse:\n",
    "    def __init__(self,name,expression,mutation,CNV,HLA,fusion):\n",
    "        self.name=name\n",
    "        self.expression=expression\n",
    "        self.mutation=mutation\n",
    "        self.CNV=CNV\n",
    "        self.HLA=HLA\n",
    "        self.fusion=fusion\n",
    "\n",
    "tmp_database={}\n",
    "tmp_database['PDX']=innerDatabse(name='PDX',\n",
    "                                 expression='/OIU/innerNGSresults/filterRNAseq/',\n",
    "                                mutation='/OIU/innerNGSresults/filterWXS/',\n",
    "                                CNV='/OIU/innerNGSresults/CNV_cnvkit/',\n",
    "                                HLA='/OIU/innerNGSresults/HLA/',\n",
    "                                fusion='/OIU/innerNGSresults/fusion_starfusion_formatPDX/')\n",
    "tmp_database['cancer_cell_line']=innerDatabse(name='cancer cell line',\n",
    "                                 expression='/OIU/innerNGSresults/CellLineDatasets/expression_format_TPM/',\n",
    "                                mutation='/OIU/innerNGSresults/CellLineDatasets/mutation_format/',\n",
    "                                CNV='/OIU/innerNGSresults/CellLineDatasets/cnv_format/',\n",
    "                                HLA='/OIU/innerNGSresults/CellLineDatasets/HLA_HD/',\n",
    "                                fusion='/OIU/innerNGSresults/CellLineDatasets/fusion_starfusion/')\n",
    "\n",
    "tmp_database['syngeneic']=innerDatabse(name='syngeneic models',\n",
    "                                 expression='/OIU/innerNGSresults/Syngeneic/expression/',\n",
    "                                mutation='/OIU/innerNGSresults/Syngeneic/mutation/',\n",
    "                                CNV='',\n",
    "                                HLA='',\n",
    "                                fusion='')\n",
    "\n",
    "model = pipeline(\"question-answering\",\n",
    "                 model=\"./tmp/finetuned-roberta-base-squad2_wuxi1\",\n",
    "                 tokenizer=\"./tmp/finetuned-roberta-base-squad2_wuxi1\")\n",
    "model_dict=get_dict('./model_dict')\n",
    "datatype_dict=get_dict('./datatype_dict')\n",
    "\n",
    "app = Flask(__name__)\n",
    "@app.route('/predict', methods=['POST'])\n",
    "\n",
    "def predict():\n",
    "    # Command-line argument parser\n",
    "    #parser = argparse.ArgumentParser(description=\"search NGS data based on your input\")\n",
    "    #parser.add_argument('--question', type=str, required=True, help=\"your query question: 在我们PDX模型中检索EGFR突变数据\")\n",
    "    #args = parser.parse_args()\n",
    "\n",
    "    # Use the example function\n",
    "    #input_contest=args.question\n",
    "    # 获取请求中的数据（假设为 JSON 格式）\n",
    "    data = request.json\n",
    "    #print(data)\n",
    "    #print(data['input'])#input_contest=data['input']\n",
    "    input_contest=data['input']\n",
    "\n",
    "    assert input_contest, \"Your input not avaliable, please check your question\"\n",
    "    models,genes,datatypes=get_inputs(input_contest)\n",
    "\n",
    "    res = 1 if models in model_dict and genes and datatypes in datatype_dict else 0\n",
    "    if res:\n",
    "        outs = str(uuid.uuid4()) + '_result.tsv'\n",
    "        search(models,genes,datatypes,outs)\n",
    "        check_status=\"\\t\".join([input_contest,datatype_dict[datatypes],models,genes,datatypes,str(res)])\n",
    "        write_out(check_status)\n",
    "        return send_file(outs,as_attachment=True)\n",
    "    else:\n",
    "        check_status=\"\\t\".join([input_contest,'-',models,genes,datatypes,str(res)])\n",
    "        write_out(check_status)\n",
    "        return 'Data received, but search failed! \\n Thanks for your support', 200\n",
    "\n",
    "# Ensure the script runs only when executed directly\n",
    "if __name__ == \"__main__\":\n",
    "    #main()\n",
    "    app.run(host='0.0.0.0', port=5000)\n",
    "\n",
    "\n",
    "'''\n",
    "客户端python脚本\n",
    "\n",
    "import requests\n",
    " \n",
    "data_to_submit = {\n",
    "    'input': '哪些PDX模型中有EGFR发生基因融合'\n",
    "}\n",
    "\n",
    "# 发送 POST 请求并获取响应\n",
    "response=requests.post('http://localhost:5000/predict', json=data_to_submit)\n",
    "\n",
    "# 检查响应状态码\n",
    "if response.status_code == 200:\n",
    "    # 将文件保存到本地\n",
    "    with open('search_result.tsv', 'wb') as f:\n",
    "        f.write(response.content)\n",
    "    print('文件已成功下载并保存为 search_result.tsv')\n",
    "else:\n",
    "    print('请求失败，状态码:', response.status_code)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4cb25b4-5a99-4657-a62a-e7bdbca8b644",
   "metadata": {},
   "source": [
    "#### 客户端脚本\n",
    "\n",
    "`search_NGS.py`\n",
    "\n",
    "+ 使用示例\n",
    "\n",
    "```sh\n",
    "python search_NGS.py --question 在小鼠模型中找一些kras突变数据\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5615602-22d5-4b03-a900-d4a1228bc486",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on 2024.10\n",
    "@author: Qiangqiang Fan\n",
    "Description: 主要用于检索PDX,细胞系,Syngeneic模型的突变、表达等数据\n",
    "\"\"\"\n",
    "\n",
    "# Import necessary modules\n",
    "import os\n",
    "import sys\n",
    "import argparse  # For handling command-line arguments\n",
    "import requests\n",
    "\n",
    "\n",
    "def search(context):\n",
    "\n",
    "    data_to_submit = {'input': context}\n",
    "    response=requests.post('http://10.111.17.67:5000/predict', json=data_to_submit)\n",
    "\n",
    "    # 检查响应状态码\n",
    "    if response.status_code == 200:\n",
    "        # 将文件保存到本地\n",
    "        with open('search_result.tsv', 'wb') as f:\n",
    "            f.write(response.content)\n",
    "        print('文件已成功下载并保存为 search_result.tsv')\n",
    "    else:\n",
    "        print('请求失败，状态码:', response.status_code)\n",
    "\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser(description=\"search NGS data based on your input\")\n",
    "    parser.add_argument('--question', type=str, required=True, help=\"your query question: 在我们PDX模型中检索EGFR突变数据\")\n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    # Use the example function\n",
    "    input_contest=args.question\n",
    "    search(input_contest)\n",
    "\n",
    "# Ensure the script runs only when executed directly\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "\n",
    "'''\n",
    "一些成功的示例：\n",
    "python search_NGS.py --question 在pdx模型中找一下kras突变数据\n",
    "python search_NGS.py --question 哪些细胞系模型中有egfr突变数据呢\n",
    "python search_NGS.py --question 在小鼠模型中找一些kras突变数据\n",
    "python search_NGS.py --question 在小鼠模型中找一些kras,egfr突变数据\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "401aa1b6-9e8d-47bc-b159-813230af93a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21639bd2-f4e6-4b9b-85ff-5c9ba6aa958f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c55a0dc8-e80a-402e-b3d4-4e551705ca64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "608cc77a-42f8-4b11-af0c-3d9ad97fe3d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a61194d-3c46-4277-9662-7619513bc5b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99d3aa31-1a1f-4e6b-a83c-5d565faba8e6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
